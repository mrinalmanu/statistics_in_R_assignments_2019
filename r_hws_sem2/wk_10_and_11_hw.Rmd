---
title: "wk_10_and_11_hw"
author: "Mrinal Vashisth"
date: "5/29/2019"
output: html_document
---

```{r,  echo=FALSE}
library(dplyr)
library(tidyr)
library(ggplot2)
library(Hmisc)
# install.packages('corrplot')
library(corrplot)
library(data.table)
#install.packages('DataExplorer')
library(DataExplorer)
#install.packages("GGally")
library(GGally)
library(MASS)       # ginv -- coefficient estimation
library(splines)    # ns, bs -- spline curves
library(multcomp)   # glht -- linear hypotheses
#BiocManager::install('edgeR')
library(edgeR)      # cpm, etc -- RNA-Seq normalization
library(limma)      # lmFit, etc -- fitting many models
library(tidyverse)  # working with data frames, plotting
library(MASS)
#install.packages('MuMIn')
library(MuMIn)
library(caTools)
library(ggfortify)
```

```{r}


# Anscombe’s data set
# Scatter plot facetted by set

anscombe_data <- readRDS('/home/manu/Downloads/anscombe.rds')

p <- ggplot(data = anscombe_data, aes(x, y, color = as.factor(set))) + 
  geom_point() + 
  scale_color_brewer(type = "qual", palette = "Set2") 

p + facet_wrap(.~set)

```

```{r}
# Pearson’s correlation by set, and non-parametric, and p-value

dt <- data.table(anscombe_data)
dtCor <- dt[, .(mCor = cor(x,y)), by=set]

# Add geom_smooth() to the plot

l <- p + geom_smooth(color="grey",size=0.5,method="lm")

l + facet_wrap(.~set)

# Summary calculation (mean, sd) grouped by set
dt_summary <- dt[, .(mean(x), mean(y), sd(x), sd(y)), by=set]
colnames(dt_summary) <- c('Sey', 'mean x', 'mean y', 'sd x', 'sd y')
dt_summary

## another silly method
# calc_mean <- aggregate(. ~set, anscombe_data, mean)
# calc_sd <- aggregate(. ~set, anscombe_data, sd)
# calc_sd['set'] <- NULL
# 
# res <- data.frame(calc_mean, calc_sd)
# colnames(res) <- c('Set', 'mean_of_x', 'mean_of_y', 'sd_of_x', 'sd_of_y')


```

```{r}

# working with airquality dataset

airquality <- read.csv("~/Desktop/AirQualityUCI/AirQualityUCI.csv", header=TRUE, sep=";", na.strings=c("-200", "-200,0"))
# -200 is missing values in the data, the data is not clean after all
airquality  <- data.frame(airquality)
airquality$X <- NULL
airquality$X.1 <- NULL


#View(airquality)

str(airquality)



```

```{r}
# new_airq <- dplyr::select_if(airquality, is.numeric)

new_airq <- airquality
new_airq$Date <- NULL
new_airq$Time <- NULL
# two line dancing to convert all factors into integers
new_airq <- data.matrix(new_airq)
new_airq <- data.frame(new_airq)
str(new_airq)
```

```{r}
# let's explore the variables
final_air <- na.omit(new_airq)

plot_density(final_air)

# let's plot a PCA
wox <- prcomp(final_air)
autoplot(wox, label = FALSE, frame = TRUE, frame.type = 'norm')
### Not very informative

# let's plot a violin to check the scale of the variables

ggplot(stack(data.frame(final_air)), aes(x = ind, y = values)) +
    geom_violin() + theme_classic()

```

```{r}

# All correlations matrix using ggcorr

ggcorr(new_airq, palette = "RdBu", label = TRUE)

summary(new_airq)

# we see that NMHC.GT. has 8557 missing values, this column is bad, we should remove it

new_airq$NMHC.GT. <- NULL

```


```{r}
# leniar models
# Let's try to predict C6H6.GT. using all variables

fit <- lm(C6H6.GT. ~ .-C6H6.GT., data=final_air)

summary(fit)

# but we kind of knew this answer, as we saw in the correlation matrix


```

```{r}

# function for plotting regression curve
ggplotRegression <- function (some_name) {

ggplot(fit$model, aes_string(x = names(fit$model)[2], y = names(fit$model)[1])) + 
  geom_point() +
  stat_smooth(method = "lm", col = "red") +
  labs(title = paste("Adj R2 = ",signif(summary(fit)$adj.r.squared, 5),
                     "Intercept =",signif(fit$coef[[1]],5 ),
                     " Slope =",signif(fit$coef[[2]], 5),
                     " P =",signif(summary(fit)$coef[2,4], 5)))
}


```

```{r}
# of course we can run an all possible simple leniar regression models for pairs of columns from a dataframe but let's work with dredge

######
# backward elemination with full model selection
####

# Backwards elimination

options(na.action = "na.fail")
global_model_raw <- lm(C6H6.GT. ~ ., data= final_air)

global_model <- step(global_model_raw, direction = "backward")

# dredge full model selection

combinations <- dredge(global_model)
all_coeff <- coefTable(combinations)

# as we can see that this is a large list, let's get all coefficients and shortlist them

cool_models <- subset(combinations, delta < 4)
cool_coeff <- coefTable(cool_models)

# Visualize the model selection table:

plot(cool_models, labAsExpr = TRUE)

#'Best' model
summary(get.models(cool_models, 1)[[1]])

best_lm_fit <- get.models(cool_models, 1)[[1]]
# Adjusted R-squared:  0.093
# We can see that the best model is SUPER BAD
# Maybe there isn't a leniar relationship

ggplotRegression(best_lm_fit)


```


```{r}
# let's work with best_lm_fit

# Splitting the dataset into the Training set and Test set

dataset <- final_air

split = sample.split(dataset, SplitRatio = 2/3)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)

# Regressor is our model_of_intrest here best_lm_fit
regressor = best_lm_fit
y_pred = predict(regressor, newdata = test_set)

plot(regressor)

# after looking at the plot it has become clearer that we need to modify our equation
```

```{r}

###################################################
#                                   TASK 11
###################################################

# let's modify our regressor and do some polynomial fitting

# CO.GT.         -2.905866   0.282316 -10.293  < 2e-16 ***
# PT08.S4.NO2.    0.091122   0.019176   4.752 2.05e-06 ***
# PT08.S5.O3.    -0.050504   0.011582  -4.361 1.32e-05 ***
# RH              0.059006   0.012865   4.587 4.58e-06 ***
# AH             -0.007845   0.001976  -3.971 7.24e-05 ***


# Splitting the dataset into the Training set and Test set

new_dataset <- final_air

split = sample.split(new_dataset, SplitRatio = 2/3)
training_set = subset(new_dataset, split == TRUE)
test_set = subset(new_dataset, split == FALSE)

# Fitting and predicting

poly_reg <- lm(C6H6.GT. ~ poly(CO.GT. + PT08.S4.NO2. +  PT08.S5.O3., 9) + poly(RH + AH, 2),
              data = training_set)
summary(poly_reg)
#summary(poly_reg)
y_pred <- predict(poly_reg, data = test_set)

# Plotting

plot(poly_reg)


```






